{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanha2301/AIR-absa-rt/blob/main/lixicon_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtP1vTkaPoT4",
        "outputId": "e978d5f5-c641-4bb6-fb37-e7a6f16d13a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-8.3.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from underthesea) (8.3.1)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.12/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from underthesea) (2.32.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.5.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from underthesea) (6.0.3)\n",
            "Collecting underthesea_core==1.0.5 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.5-cp312-cp312-manylinux2010_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.8->underthesea) (2025.11.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2025.11.12)\n",
            "Downloading underthesea-8.3.0-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading underthesea_core-1.0.5-cp312-cp312-manylinux2010_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.4/978.4 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: underthesea_core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.11 underthesea-8.3.0 underthesea_core-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "DAyvnoosQC2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 1. Đường dẫn file cấu hình (KHÔNG dùng negative_words_vi.txt)\n",
        "# =========================\n",
        "\n",
        "LEXICON_PATH = \"VietSentiWordnet_Ver1.3.5.txt\"\n",
        "STOPWORDS_PATH = \"vietnamese-stopwords-dash.txt\"\n",
        "NEGATION_MARKERS_PATH = \"vietnamese_negation_markers_full.txt\"  # list từ phủ định\n",
        "\n",
        "\n",
        "# Ngưỡng phân lớp 3 nhãn\n",
        "POS_THRESHOLD = 0.05\n",
        "NEG_THRESHOLD = -0.05\n",
        "\n",
        "# Từ nhấn mạnh (intensifiers)\n",
        "INTENSIFIERS = {\n",
        "    \"rất\": 1.5,\n",
        "    \"cực_kì\": 1.8,\n",
        "    \"cực_kỳ\": 1.8,\n",
        "    \"cực_kỳ\": 1.8,\n",
        "    \"quá\": 1.3,\n",
        "    \"siêu\": 1.8,\n",
        "    \"vô_cùng\": 1.8,\n",
        "    \"khá\": 1.2,\n",
        "    \"hơi\": 0.7,   # làm nhẹ cảm xúc\n",
        "}\n",
        "\n",
        "# Số token nhìn lùi lại để bắt phủ định\n",
        "NEGATION_WINDOW = 3"
      ],
      "metadata": {
        "id": "Phnqcuq8QKDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 2. Load lexicon, stopwords, negations\n",
        "# =========================\n",
        "\n",
        "def load_vnsenti_lexicon(path: str) -> dict:\n",
        "    \"\"\"Đọc VietSentiWordNet và trả về: word -> sentiment_score\"\"\"\n",
        "    lexicon_sum = defaultdict(float)\n",
        "    lexicon_count = defaultdict(int)\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "\n",
        "            pos_tag, synset_id, pos_score, neg_score, synset_terms = parts[:5]\n",
        "\n",
        "            try:\n",
        "                pos_score = float(pos_score.replace(\",\", \".\"))\n",
        "                neg_score = float(neg_score.replace(\",\", \".\"))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            score = pos_score - neg_score\n",
        "\n",
        "            for term in synset_terms.split():\n",
        "                lemma = term.split(\"#\")[0].strip()\n",
        "                if not lemma:\n",
        "                    continue\n",
        "                lexicon_sum[lemma] += score\n",
        "                lexicon_count[lemma] += 1\n",
        "\n",
        "    lexicon = {}\n",
        "    for w, s in lexicon_sum.items():\n",
        "        lexicon[w] = s / lexicon_count[w]\n",
        "    return lexicon\n",
        "\n",
        "\n",
        "def load_stopwords(path: str) -> set:\n",
        "    sw = set()\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            w = line.strip()\n",
        "            if w:\n",
        "                sw.add(w)\n",
        "    return sw\n",
        "\n",
        "\n",
        "def load_negation_markers(path: str) -> set:\n",
        "    markers = set()\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            w = line.strip()\n",
        "            if w:\n",
        "                markers.add(w)\n",
        "    return markers\n",
        "\n",
        "\n",
        "def build_full_lexicon(main_path: str = LEXICON_PATH) -> dict:\n",
        "    \"\"\"Chỉ dùng VietSentiWordNet (không gộp negative_words_vi.txt).\"\"\"\n",
        "    print(\">>> Đang load VietSentiWordNet...\")\n",
        "    lex_main = load_vnsenti_lexicon(main_path)\n",
        "    print(f\"    Số từ trong VietSentiWordNet: {len(lex_main)}\")\n",
        "    return lex_main\n",
        "\n"
      ],
      "metadata": {
        "id": "CxCTzENwQNVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 3. Tiền xử lý & tách từ\n",
        "# =========================\n",
        "\n",
        "URL_RE = re.compile(r\"http\\S+|www\\.\\S+\")\n",
        "NON_ALPHA_RE = re.compile(r\"[^0-9a-zA-ZÀ-ỹ_ ]+\")\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    text = text.lower()\n",
        "    text = URL_RE.sub(\" \", text)\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = NON_ALPHA_RE.sub(\" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize_vi(text: str) -> list:\n",
        "    return word_tokenize(text, format=\"text\").split()\n",
        "\n",
        "\n",
        "def preprocess_and_tokenize(text: str, stopwords: set, negation_words: set) -> list:\n",
        "    text = normalize_text(text)\n",
        "    tokens = tokenize_vi(text)\n",
        "\n",
        "    filtered = []\n",
        "    for t in tokens:\n",
        "        t = t.strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        if t in negation_words:\n",
        "            filtered.append(t)\n",
        "        else:\n",
        "            if t not in stopwords:\n",
        "                filtered.append(t)\n",
        "\n",
        "    return filtered\n",
        "\n"
      ],
      "metadata": {
        "id": "RgubfhXOQP2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 4. Tính điểm sentiment\n",
        "# =========================\n",
        "\n",
        "def sentiment_score(\n",
        "    tokens: list,\n",
        "    lexicon: dict,\n",
        "    negation_words: set,\n",
        "    use_negation: bool = True,\n",
        "    use_intensifier: bool = True,\n",
        ") -> float:\n",
        "    total = 0.0\n",
        "\n",
        "    for i, tok in enumerate(tokens):\n",
        "        # Từ phủ định chỉ dùng để đảo dấu, không tự mang score\n",
        "        if tok in negation_words:\n",
        "            continue\n",
        "\n",
        "        if tok not in lexicon:\n",
        "            continue\n",
        "\n",
        "        score = lexicon[tok]\n",
        "\n",
        "        # Intensifier ngay trước\n",
        "        if use_intensifier and i > 0:\n",
        "            prev_tok = tokens[i - 1]\n",
        "            if prev_tok in INTENSIFIERS:\n",
        "                score *= INTENSIFIERS[prev_tok]\n",
        "\n",
        "        # Phủ định trong cửa sổ trước đó\n",
        "        if use_negation:\n",
        "            for j in range(max(0, i - NEGATION_WINDOW), i):\n",
        "                if tokens[j] in negation_words:\n",
        "                    score *= -1\n",
        "                    break\n",
        "\n",
        "        total += score\n",
        "\n",
        "    return total\n",
        "\n",
        "\n",
        "def score_to_label(\n",
        "    score: float,\n",
        "    pos_threshold: float = POS_THRESHOLD,\n",
        "    neg_threshold: float = NEG_THRESHOLD,\n",
        ") -> str:\n",
        "    if score > pos_threshold:\n",
        "        return \"positive\"\n",
        "    elif score < neg_threshold:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "\n",
        "def predict_sentiment(\n",
        "    text: str,\n",
        "    lexicon: dict,\n",
        "    stopwords: set,\n",
        "    negation_words: set,\n",
        ") -> tuple:\n",
        "    tokens = preprocess_and_tokenize(text, stopwords, negation_words)\n",
        "    s = sentiment_score(tokens, lexicon, negation_words)\n",
        "    label = score_to_label(s)\n",
        "    return label, s\n"
      ],
      "metadata": {
        "id": "n3rCcUXOf_N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 5. Evaluate trên dataset CSV\n",
        "# =========================\n",
        "\n",
        "def evaluate_on_csv(\n",
        "    csv_path: str,\n",
        "    text_col: str = \"text\",\n",
        "    label_col: str = \"label\",\n",
        "    lexicon_path: str = LEXICON_PATH,\n",
        "    stopwords_path: str = STOPWORDS_PATH,\n",
        "    negation_markers_path: str = NEGATION_MARKERS_PATH,\n",
        "):\n",
        "    print(\">>> Đang build lexicon từ VietSentiWordNet...\")\n",
        "    lexicon = build_full_lexicon(lexicon_path)\n",
        "\n",
        "    print(\">>> Đang load stopwords...\")\n",
        "    stopwords = load_stopwords(stopwords_path)\n",
        "    print(f\"    Số stopwords: {len(stopwords)}\")\n",
        "\n",
        "    print(\">>> Đang load từ phủ định...\")\n",
        "    negation_words = load_negation_markers(negation_markers_path)\n",
        "    print(f\"    Số từ phủ định: {len(negation_words)}\")\n",
        "\n",
        "    print(\">>> Đang load dataset...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    assert text_col in df.columns, f\"Không thấy cột text: {text_col}\"\n",
        "    assert label_col in df.columns, f\"Không thấy cột label: {label_col}\"\n",
        "\n",
        "    y_true = df[label_col].astype(str).tolist()\n",
        "    y_pred = []\n",
        "    scores = []\n",
        "\n",
        "    print(\">>> Đang dự đoán trên dataset...\")\n",
        "    for text in df[text_col]:\n",
        "        label, score = predict_sentiment(text, lexicon, stopwords, negation_words)\n",
        "        y_pred.append(label)\n",
        "        scores.append(score)\n",
        "\n",
        "    df[\"pred_label\"] = y_pred\n",
        "    df[\"pred_score\"] = scores\n",
        "\n",
        "    print(\"\\n=== Classification report (macro) ===\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Confusion matrix (rows: true, cols: pred) ===\")\n",
        "    labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    print(\"labels:\", labels)\n",
        "    print(cm)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "OtNE_Ia-QTZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# 6. Demo nhanh\n",
        "# =========================\n",
        "\n",
        "\n",
        "print(\">>> Khởi tạo lexicon, stopwords, negation words để demo...\")\n",
        "lexicon = build_full_lexicon(LEXICON_PATH)\n",
        "stopwords = load_stopwords(STOPWORDS_PATH)\n",
        "negation_words = load_negation_markers(NEGATION_MARKERS_PATH)\n",
        "\n",
        "examples = [\n",
        "    \"Điện thoại này rất tốt, pin trâu và màn hình đẹp.\",\n",
        "    \"Sản phẩm quá tệ, dùng được vài hôm là hỏng.\",\n",
        "    \"Tạm ổn, không có gì đặc biệt.\",\n",
        "    \"Chất lượng không tốt như mong đợi.\",\n",
        "    \"Mình không hề thất vọng, thậm chí rất hài lòng.\",\n",
        "    \"Hoàn toàn không đáng tiền, quá thất vọng.\",\n",
        "    \"Không tệ như mình nghĩ, dùng cũng ổn.\",\n",
        "]\n",
        "\n",
        "print(\"\\n=== DEMO CÂU LẺ ===\")\n",
        "for sent in examples:\n",
        "    label, score = predict_sentiment(sent, lexicon, stopwords, negation_words)\n",
        "    print(f\"{sent} -> {label} (score={score:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "mY5JEjRbQYvo",
        "outputId": "af5386f9-c3a2-4fa2-e9e8-60ed4acdf238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Khởi tạo lexicon, stopwords, negation words để demo...\n",
            ">>> Đang load VietSentiWordNet...\n",
            "    Số từ trong VietSentiWordNet: 1226\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vietnamese_negation_markers_full.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3412362975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_full_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEXICON_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOPWORDS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnegation_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_negation_markers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNEGATION_MARKERS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m examples = [\n",
            "\u001b[0;32m/tmp/ipython-input-1998137427.py\u001b[0m in \u001b[0;36mload_negation_markers\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_negation_markers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmarkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vietnamese_negation_markers_full.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ============================\n",
        "# 1. STRING SUBSEQUENCE KERNEL\n",
        "# ============================\n",
        "\n",
        "def string_kernel(s, t, max_len=5, lam=0.5):\n",
        "    \"\"\"\n",
        "    s, t: strings\n",
        "    max_len: length of subsequences\n",
        "    lam: decay parameter\n",
        "    \"\"\"\n",
        "    s = s.replace(\" \", \"\")\n",
        "    t = t.replace(\" \", \"\")\n",
        "\n",
        "    K = 0.0\n",
        "    for L in range(1, max_len + 1):\n",
        "        for i in range(len(s) - L + 1):\n",
        "            subseq = s[i:i+L]\n",
        "            count = t.count(subseq)\n",
        "            if count > 0:\n",
        "                K += (lam ** L) * count\n",
        "    return K\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 2. TÍNH POS SCORE GIỐNG VSWN\n",
        "# ============================\n",
        "\n",
        "def senti_score(gloss, pos_seeds, neg_seeds):\n",
        "    sim_pos = sum(string_kernel(gloss, g) for g in pos_seeds)\n",
        "    sim_neg = sum(string_kernel(gloss, g) for g in neg_seeds)\n",
        "\n",
        "    if sim_pos + sim_neg == 0:\n",
        "        return 0.0, 0.0   # edge case\n",
        "\n",
        "    Pos = sim_pos / (sim_pos + sim_neg)\n",
        "    Neg = sim_neg / (sim_pos + sim_neg)\n",
        "    return Pos, Neg\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 3. TEST VỚI GLOSS \"HẤP DẪN\"\n",
        "# ============================\n",
        "\n",
        "gloss_hap_dan = \"\"\"\n",
        "thích nhìn, say mê vẻ đẹp; quần áo hấp dẫn; mô tả quyển sách vở hình minh họa hấp dẫn\n",
        "\"\"\"\n",
        "\n",
        "# Tập seed POS mẫu (đơn giản hóa để minh hoạ)\n",
        "pos_seeds = [\n",
        "    \"đẹp, thu hút, lôi cuốn, quyến rũ\",\n",
        "    \"dễ chịu, hấp dẫn\",\n",
        "    \"say mê, thích thú\"\n",
        "]\n",
        "\n",
        "# Tập seed NEG mẫu (đơn giản hóa)\n",
        "neg_seeds = [\n",
        "    \"xấu, tệ hại, kinh khủng\",\n",
        "    \"khó chịu, kém hấp dẫn\"\n",
        "]\n",
        "\n",
        "Pos, Neg = senti_score(gloss_hap_dan, pos_seeds, neg_seeds)\n",
        "\n",
        "print(\"POS SCORE =\", Pos)\n",
        "print(\"NEG SCORE =\", Neg)\n"
      ],
      "metadata": {
        "id": "3qDWOeWClih1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}