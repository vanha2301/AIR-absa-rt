{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanha2301/AIR-absa-rt/blob/main/lexicon_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAGrsSTI3woA"
      },
      "source": [
        "# Lexicon-based Sentiment (VI) — VietSentiWordNet + NegDict + SacThaiDict\n",
        "\n",
        "Notebook **cơ bản** (không argparse / không CLI).  \n",
        "Mục tiêu: đọc 3 file `.txt` bạn đưa và tính sentiment theo luật:\n",
        "\n",
        "- **VietSentiWordnet_Ver1.3.5.txt** → điểm từ (pos - neg)\n",
        "- **NegDict.txt** → từ phủ định (không, chẳng, chưa, ...)\n",
        "- **SacThaiDict.txt** → từ nhấn mạnh / mức độ (rất, quá, cực_kỳ, ...)\n",
        "\n",
        "> Lưu ý: `underthesea.word_tokenize(format=\"text\")` sẽ nối từ ghép bằng dấu `_`\n",
        "> (ví dụ: `tốt nhất` → `tốt_nhất`). Vì vậy notebook chuẩn hoá lexicon để match kiểu token này. citeturn0search4\n"
      ],
      "id": "eAGrsSTI3woA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwNal47o3woF"
      },
      "source": [
        "## 0) Chuẩn bị file\n",
        "\n",
        "Đặt các file này **cùng thư mục** với notebook:\n",
        "\n",
        "- `VietSentiWordnet_Ver1.3.5.txt`\n",
        "- `NegDict.txt`\n",
        "- `SacThaiDict.txt`\n",
        "\n",
        "Nếu bạn để chỗ khác thì sửa lại biến `LEXICON_PATH / NEG_PATH / INTENS_PATH` ở cell bên dưới.\n"
      ],
      "id": "uwNal47o3woF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_u_q1TM3woH"
      },
      "source": [
        "# (Tuỳ chọn) Cài thư viện nếu máy bạn chưa có\n",
        "!pip -q install underthesea pandas\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "e_u_q1TM3woH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Ga1AJ13woK"
      },
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from underthesea import word_tokenize\n",
        "except Exception:\n",
        "    word_tokenize = None\n",
        "\n",
        "# ==== ĐƯỜNG DẪN 3 FILE (sửa ở đây nếu cần) ====\n",
        "BASE_DIR = Path(\".\")  # thư mục notebook\n",
        "LEXICON_PATH = BASE_DIR / \"VietSentiWordnet_Ver1.3.5.txt\"\n",
        "NEG_PATH = BASE_DIR / \"NegDict.txt\"\n",
        "INTENS_PATH = BASE_DIR / \"SacThaiDict.txt\"\n",
        "\n",
        "# ==== normalize / tokenize ====\n",
        "URL_RE = re.compile(r\"http\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
        "NON_ALPHA_RE = re.compile(r\"[^0-9a-zA-ZÀ-ỹ_ ]+\")\n",
        "\n",
        "def nfc(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFC\", s)\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = nfc(text).lower()\n",
        "    text = URL_RE.sub(\" \", text)\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = NON_ALPHA_RE.sub(\" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_vi(text: str) -> List[str]:\n",
        "    text = normalize_text(text)\n",
        "    if word_tokenize is None:\n",
        "        return text.split()\n",
        "    # underthesea format=\"text\" sẽ nối cụm từ bằng \"_\"\n",
        "    return nfc(word_tokenize(text, format=\"text\")).split()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "O6Ga1AJ13woK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2y9LyQa3woM"
      },
      "source": [
        "def load_word_list(path: Path) -> Set[str]:\n",
        "    words: Set[str] = set()\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "        for line in f:\n",
        "            w = nfc(line.strip()).lower()\n",
        "            if not w or w.startswith(\"#\"):\n",
        "                continue\n",
        "            # chuẩn hoá \"tốt nhất\" -> \"tốt_nhất\" để match tokenization\n",
        "            w = w.replace(\" \", \"_\")\n",
        "            words.add(w)\n",
        "    return words\n",
        "\n",
        "def load_intensifiers(path: Path) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    SacThaiDict.txt thường là: word<tab>weight\n",
        "    Nhưng cũng có thể là: word weight  (space)\n",
        "    \"\"\"\n",
        "    intens: Dict[str, float] = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "        for raw in f:\n",
        "            line = nfc(raw.strip())\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            parts = re.split(r\"\\s+\", line)\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            word = parts[0].lower().replace(\" \", \"_\")\n",
        "            try:\n",
        "                weight = float(parts[1].replace(\",\", \".\"))\n",
        "            except ValueError:\n",
        "                continue\n",
        "            intens[word] = weight\n",
        "    return intens\n",
        "\n",
        "def _parse_synset_terms(synset_terms: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    SynsetTerms ví dụ: \"tốt_hơn#3 tốt nhất#2\"\n",
        "    -> cần tách theo mẫu kết thúc \"#<digits>\"\n",
        "    \"\"\"\n",
        "    synset_terms = nfc(synset_terms.strip())\n",
        "    return [m.group(1).strip() for m in re.finditer(r\"(.+?#\\d+)(?=\\s+|$)\", synset_terms)]\n",
        "\n",
        "def load_vnsenti_lexicon(path: Path) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Format mỗi dòng (tab-separated):\n",
        "      POS \\t ID \\t PosScore \\t NegScore \\t SynsetTerms \\t Gloss\n",
        "    Trả về: lemma -> avg(PosScore - NegScore)\n",
        "    \"\"\"\n",
        "    lex_sum: Dict[str, float] = defaultdict(float)\n",
        "    lex_cnt: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "        for raw in f:\n",
        "            line = nfc(raw.strip())\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "            if parts[0].lower() == \"pos\":  # header (nếu có)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                pos_score = float(parts[2].replace(\",\", \".\"))\n",
        "                neg_score = float(parts[3].replace(\",\", \".\"))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            score = pos_score - neg_score\n",
        "            for term in _parse_synset_terms(parts[4]):\n",
        "                lemma = term.rsplit(\"#\", 1)[0].strip().lower()\n",
        "                if not lemma:\n",
        "                    continue\n",
        "                lemma = nfc(lemma.replace(\" \", \"_\"))\n",
        "                lex_sum[lemma] += score\n",
        "                lex_cnt[lemma] += 1\n",
        "\n",
        "    return {w: lex_sum[w] / lex_cnt[w] for w in lex_sum.keys()}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "H2y9LyQa3woM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEmDRhLV3woN"
      },
      "source": [
        "def preprocess(text: str) -> List[str]:\n",
        "    tokens = tokenize_vi(text)\n",
        "    return [nfc(t).lower().strip() for t in tokens if t.strip()]\n",
        "\n",
        "def sentiment_score(\n",
        "    tokens: List[str],\n",
        "    lexicon: Dict[str, float],\n",
        "    neg_words: Set[str],\n",
        "    intens: Dict[str, float],\n",
        "    neg_window: int = 3,\n",
        ") -> float:\n",
        "    total = 0.0\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if tok in neg_words:\n",
        "            continue\n",
        "\n",
        "        s = lexicon.get(tok)\n",
        "        if s is None:\n",
        "            continue\n",
        "\n",
        "        # intensifier ngay trước\n",
        "        if i > 0 and tokens[i - 1] in intens:\n",
        "            s *= intens[tokens[i - 1]]\n",
        "\n",
        "        # có phủ định trong cửa sổ phía trước\n",
        "        start = max(0, i - neg_window)\n",
        "        if any(t in neg_words for t in tokens[start:i]):\n",
        "            s *= -1\n",
        "\n",
        "        total += s\n",
        "\n",
        "    return float(total)\n",
        "\n",
        "def score_to_label(score: float, pos_th: float = 0.05, neg_th: float = -0.05) -> str:\n",
        "    if score > pos_th:\n",
        "        return \"positive\"\n",
        "    if score < neg_th:\n",
        "        return \"negative\"\n",
        "    return \"neutral\"\n",
        "\n",
        "def predict_sentiment(\n",
        "    text: str,\n",
        "    lexicon: Dict[str, float],\n",
        "    neg_words: Set[str],\n",
        "    intens: Dict[str, float],\n",
        "    pos_th: float = 0.05,\n",
        "    neg_th: float = -0.05,\n",
        "    neg_window: int = 3,\n",
        ") -> Tuple[str, float]:\n",
        "    tokens = preprocess(text)\n",
        "    sc = sentiment_score(tokens, lexicon, neg_words, intens, neg_window=neg_window)\n",
        "    return score_to_label(sc, pos_th, neg_th), sc\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "kEmDRhLV3woN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5e4J1de3woN"
      },
      "source": [
        "# ==== Load 3 resources ====\n",
        "assert LEXICON_PATH.exists(), f\"Không thấy file: {LEXICON_PATH}\"\n",
        "assert NEG_PATH.exists(), f\"Không thấy file: {NEG_PATH}\"\n",
        "assert INTENS_PATH.exists(), f\"Không thấy file: {INTENS_PATH}\"\n",
        "\n",
        "lexicon = load_vnsenti_lexicon(LEXICON_PATH)\n",
        "neg_words = load_word_list(NEG_PATH)\n",
        "intens = load_intensifiers(INTENS_PATH)\n",
        "\n",
        "print(\"Lexicon size:\", len(lexicon))\n",
        "print(\"Neg words:\", len(neg_words))\n",
        "print(\"Intensifiers:\", len(intens))\n",
        "\n",
        "# ==== Demo nhanh ====\n",
        "examples = [\n",
        "    \"Điện thoại này rất tốt, pin trâu và màn hình đẹp.\",\n",
        "    \"Sản phẩm quá tệ, dùng được vài hôm là hỏng.\",\n",
        "    \"Tạm ổn, không có gì đặc biệt.\",\n",
        "    \"Chất lượng không tốt như mong đợi.\",\n",
        "    \"Mình không hề thất vọng, thậm chí rất hài lòng.\",\n",
        "    \"Hoàn toàn không đáng tiền, quá thất vọng.\",\n",
        "    \"Không tệ như mình nghĩ, dùng cũng ổn.\",\n",
        "]\n",
        "\n",
        "for s in examples:\n",
        "    lab, sc = predict_sentiment(s, lexicon, neg_words, intens)\n",
        "    print(f\"- {s} -> {lab:8s} | score={sc:.3f}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "-5e4J1de3woN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyc2oScj3woO"
      },
      "source": [
        "def predict_on_csv(\n",
        "    csv_path: str,\n",
        "    text_col: str = \"text\",\n",
        "    out_path: Optional[str] = None,\n",
        "    pos_th: float = 0.05,\n",
        "    neg_th: float = -0.05,\n",
        "    neg_window: int = 3,\n",
        ") -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if text_col not in df.columns:\n",
        "        raise ValueError(f\"Missing text column: {text_col}. Available: {list(df.columns)}\")\n",
        "\n",
        "    preds: List[str] = []\n",
        "    scores: List[float] = []\n",
        "\n",
        "    for txt in df[text_col].astype(str).tolist():\n",
        "        lab, sc = predict_sentiment(\n",
        "            txt, lexicon, neg_words, intens,\n",
        "            pos_th=pos_th, neg_th=neg_th, neg_window=neg_window\n",
        "        )\n",
        "        preds.append(lab)\n",
        "        scores.append(sc)\n",
        "\n",
        "    df[\"pred_label\"] = preds\n",
        "    df[\"pred_score\"] = scores\n",
        "\n",
        "    if out_path:\n",
        "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "        print(\"Saved:\", out_path)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Ví dụ:\n",
        "# df_pred = predict_on_csv(\"data.csv\", text_col=\"text\", out_path=\"data.pred.csv\")\n",
        "# df_pred.head()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "Gyc2oScj3woO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}